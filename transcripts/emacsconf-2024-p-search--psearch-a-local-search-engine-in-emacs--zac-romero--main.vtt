WEBVTT

00:00:00.960 --> 00:00:06.320
 Hello, my name is Zachary Romero and today I'll be going over PSearch, a local search engine in EMAX.

00:00:06.320 --> 00:00:15.680
 So search these days everywhere in software from text editors to IDs to most online websites

00:00:15.680 --> 00:00:22.400
 and these tools tend to fall into one of two categories. One are tools that run locally

00:00:22.960 --> 00:00:32.640
 and work by matching string to text. And the most common example, this is GRIP.

00:00:32.640 --> 00:00:39.280
 In EMAX there are a lot of extensions which provide functionality on top of these tools,

00:00:39.280 --> 00:00:47.360
 such as projectile GRIP, deadgrab, consult GRIP, and most editors have some sort of search current

00:00:47.360 --> 00:00:54.480
 project feature. And most of the time some features, some of these tools have features like

00:00:54.480 --> 00:01:00.880
 regular expressions or you can specify file extension or director you want to search in,

00:01:00.880 --> 00:01:06.640
 but features are pretty limited. And the other kind of search we use are usually hosted online.

00:01:06.640 --> 00:01:13.600
 And they usually search a vast corpus of data. And these are usually

00:01:13.600 --> 00:01:20.000
 prepared online services such as Google, GitHub, Source Graph for code.

00:01:20.000 --> 00:01:31.840
 And let's say so, the kind of search feature that editors usually have have a lot of

00:01:31.840 --> 00:01:39.440
 downsides to them. For one, a lot of times you don't know the exact searching you're searching for.

00:01:40.080 --> 00:01:46.080
 So some complicated term like this high volume demand partner, you know, do you know if it is it,

00:01:46.080 --> 00:01:52.320
 are some words abbreviated as a capitalized, is it in Kabab case, Camel case, Snake case, and

00:01:52.320 --> 00:02:03.520
 you often have to search all these variations. Another downside is that the search results returned

00:02:03.520 --> 00:02:09.440
 are contained contain a lot of noise. So for example, you may get a lot of test files,

00:02:10.400 --> 00:02:15.760
 if it if the tool hits your vendor directory, it may get a bunch of results from libraries you're

00:02:15.760 --> 00:02:24.960
 using, which is which most are not not helpful. Another downside is that the order given is well,

00:02:24.960 --> 00:02:30.240
 there's no there's no meaning to the order. It's usually just the search order that the tool

00:02:30.240 --> 00:02:38.560
 happens to look in first. Another thing is so when you're searching, you often have you often

00:02:38.560 --> 00:02:44.720
 time to have to keep the state of the searches in your head. So for example, you try one search,

00:02:44.720 --> 00:02:49.280
 you see the results, find the results you you think are relevant, keep them in your head,

00:02:49.280 --> 00:02:54.720
 run search number two, look through the results, kind of combine these result, these different

00:02:54.720 --> 00:02:58.400
 search results in your head until you get an idea of which which ones might be relevant.

00:02:58.400 --> 00:03:07.200
 Another thing is that the search primitives are fairly limited. So yeah, you can search regular

00:03:07.200 --> 00:03:16.000
 expression, but you can't really define complex things like I want to search files in this directory,

00:03:16.000 --> 00:03:21.760
 in this and this directory, and this directory, except these subdirectories and accept test files,

00:03:21.760 --> 00:03:27.840
 and I only want files with this file extension. Criteria like that are really hard to, I'm sure they're

00:03:27.840 --> 00:03:35.040
 possible in tools like grip, but they're pretty, they're pretty hard to to construct. And lastly,

00:03:35.040 --> 00:03:40.400
 there's no, there's no notion of any relevance. So all the results you get back, I mean, you don't

00:03:40.400 --> 00:03:48.800
 know, is this search more relevant? Is it twice as relevant? Is it, is it, is this, is it 100 times more

00:03:48.800 --> 00:03:59.360
 relevant? These tools usually don't provide such information. So there's a field called information

00:03:59.360 --> 00:04:04.560
 retrieval, and this deals with this exact problem. So you have lots of data you're searching for,

00:04:04.560 --> 00:04:10.400
 how do you construct a search query? How do you get results back fast? How do you rank which ones

00:04:10.400 --> 00:04:16.800
 are most relevant? And how do you, how do you evaluate your search system to see if it's getting

00:04:16.800 --> 00:04:22.800
 better or worse? So, so there's a lot of work, a lot of books written on the, on the topic of

00:04:22.800 --> 00:04:30.640
 information retrieval, and if, if one wants to improve searching any max, then drawing inspiration

00:04:30.640 --> 00:04:41.600
 from this field is, is necessary. So the first aspect of information retrieval is the index. So the,

00:04:41.600 --> 00:04:47.600
 the reverse index is what search engines use to find results really fast. And essentially, it's a,

00:04:47.600 --> 00:04:55.360
 it's a map of search term to locations where that, where that term is located. So you'll have,

00:04:55.360 --> 00:04:59.120
 all the terms, or maybe even parts of the terms, and then you'll have all the locations where they're

00:04:59.120 --> 00:05:04.320
 located. And so like any query could easily look up where things are located, join, join results

00:05:04.320 --> 00:05:14.400
 together. And, and that's how they get the results to be really fast. So for, for this project, I

00:05:14.400 --> 00:05:23.600
 decided to, for go creating an index altogether. As an index is, it's pretty complicated to maintain,

00:05:23.600 --> 00:05:28.800
 because it always has to be in sync. So anytime you open a file and save it, it would have to re-index

00:05:29.040 --> 00:05:33.600
 it, have to make sure that file is re-indexed properly. And then you have the whole issue of like, well,

00:05:33.600 --> 00:05:39.680
 if you're searching any max, like, okay, you have all these projects, this directory, that directory,

00:05:39.680 --> 00:05:46.240
 how do you know, which you always have to keep them in sync. And so it's, it's quite a hard task

00:05:46.240 --> 00:05:54.000
 to keep, to handle that. And then on the other end, tools like RIPGrep are, can search very fast,

00:05:54.720 --> 00:06:00.800
 even though they can't search, maybe, you know, they can't search, maybe on the order of tens of

00:06:00.800 --> 00:06:10.400
 thousands of repositories. For a local setting, they should be plenty fast enough. I, I benchmarked it,

00:06:10.400 --> 00:06:15.360
 and it's about like, RIPGrep, for example, is about, it's on the order of gigabytes per second.

00:06:15.360 --> 00:06:22.960
 So definitely a search of a few pretty big size repositories. So next main task, so,

00:06:23.760 --> 00:06:29.920
 we decided not to use an index. Next task is how do we rank, how do we rank search results. So there's

00:06:29.920 --> 00:06:35.920
 two main algorithms that are used these days. So the first one is TFIDF, which stands for term frequency

00:06:35.920 --> 00:06:42.400
 inverse starting frequency. And then there's BM25, which is sort of a modified TFIDF algorithm.

00:06:42.400 --> 00:06:48.640
 So TFIDF, without going into too much detail, it's essentially, it essentially multiplies two terms.

00:06:48.640 --> 00:06:53.920
 One is the term frequency, and then you multiply it by the inverse document frequency. So the term

00:06:53.920 --> 00:06:59.280
 frequency is a measure of how often that search term occurs. And the inverse document frequency

00:06:59.280 --> 00:07:06.560
 is a measure of how much information that term provides. So if the term occurs a lot, then it gets

00:07:06.560 --> 00:07:11.680
 a higher score in the term frequency section. But if it's a common word that exists in a lot of

00:07:11.680 --> 00:07:18.880
 documents, then it's inverse document frequency goes down. So it's kind of scores at less. So you'll

00:07:18.880 --> 00:07:27.360
 find that words like the in is these really common words, since they occur in they occur everywhere.

00:07:27.360 --> 00:07:32.720
 Their inverse document frequency is essentially zero. So they don't really count towards a score.

00:07:32.720 --> 00:07:37.120
 But when you have like rare words that only occur in a few documents, they're weighted a lot more.

00:07:37.120 --> 00:07:46.560
 And so the more those rare words occur, they boost the score higher. So BM25 is a modification

00:07:46.560 --> 00:07:53.200
 of this. It's essentially TF, it's essentially the previous one, except it dampens out terms that

00:07:53.200 --> 00:07:58.560
 occur more often. So imagine you have a bunch of documents. One has a term 10 times, one has a

00:07:58.560 --> 00:08:03.920
 term, that same term 100 times, another has a thousand times. You'll find that you'll see the score

00:08:04.880 --> 00:08:10.880
 dampens up as the as a number of occurrences increases. So that prevents any one term from

00:08:10.880 --> 00:08:18.400
 overpowering the score. So this is the this is the algorithm I ended up choosing for my implementation.

00:08:18.400 --> 00:08:26.320
 And so with a so with a plan of using a command line tool like ripgrap to get term occurrences.

00:08:28.640 --> 00:08:34.880
 And then using the beat like a scoring algorithm like BM25 to rank the rank the current rank the

00:08:34.880 --> 00:08:41.920
 the terms we can combine this together and create a simple search engine, a simple search mechanism.

00:08:41.920 --> 00:08:48.640
 So here we're in the directory for the EMAX source code. And let's say we want to search for

00:08:48.640 --> 00:08:55.760
 say the display code. So we we start we run the p search command opening up starting the

00:08:56.400 --> 00:09:02.400
 search engine. And it opens up and we notice it has three sections the candidate generators the

00:09:02.400 --> 00:09:10.080
 priors and the search results. So this candidate generators generates the the search space we're

00:09:10.080 --> 00:09:15.360
 looking we're we're looking on. So these are all composable and you can add as many as you want.

00:09:15.360 --> 00:09:25.120
 And so with this it specifies that here we're searching on the file system. And we're searching in

00:09:25.120 --> 00:09:32.320
 this directory. We're using the ripgrap tool to search with and we want to make sure that we're

00:09:32.320 --> 00:09:41.600
 searching only on files committed to to get. And here we see the search results. Notice here is

00:09:41.600 --> 00:09:46.640
 their their final probability. And so here notice that they're all the same and they're the same

00:09:46.640 --> 00:09:52.320
 because we don't have any search criteria specified here. So suppose we want to search for display

00:09:52.320 --> 00:10:05.920
 related code. So we add a query display. And so then it spins off the processes gets the search

00:10:05.920 --> 00:10:12.480
 term counts and calculates the new scores. And notice here that the results that come on top are

00:10:12.480 --> 00:10:20.160
 are just our first glance a pew to be relevant to display. And so remember if we compare that to

00:10:20.160 --> 00:10:27.360
 just running a ripgrap raw. Notice here we're getting five fifty three thousand results and

00:10:27.360 --> 00:10:34.240
 it's it's pretty hard to to go through these results and it makes sense of it.

00:10:34.240 --> 00:10:43.760
 So that's piece search in a nutshell. Next I wanted to talk about the story of flight

00:10:43.760 --> 00:10:51.520
 447. So flight 447 going from Rio de Janeiro to Paris crashed somewhere in the Atlantic Ocean on

00:10:51.520 --> 00:10:56.960
 June 1st 2009 killing everyone on board. Four search attempts were made to find the wreckage. None

00:10:56.960 --> 00:11:03.840
 of them were successful except the finding of some debris and a dead body. It was decided that they

00:11:03.840 --> 00:11:09.760
 wanted to they really wanted to find the wreckage to retrieve data as to why the search occurred.

00:11:11.520 --> 00:11:15.360
 And this was this occurred two years after the initial crash.

00:11:15.360 --> 00:11:23.040
 So with this next search attempt they wanted to create a probability distribution of where

00:11:23.040 --> 00:11:29.600
 the crash could be. So the only piece of concrete data they had was a GPS signal from the ship

00:11:29.600 --> 00:11:40.400
 on at 210. And containing the GPS location that the plane was at 2.98 degrees north 30.59 degrees

00:11:40.400 --> 00:11:47.360
 west and that was the only data they had to go off of. So they drew a circle around that that point

00:11:47.360 --> 00:11:55.440
 40 nautical miles right with the radius of 40 nautical miles and they assumed that anything

00:11:55.440 --> 00:12:00.000
 outside the circle would have been impossible for the ship to reach. So this was the this was a

00:12:00.000 --> 00:12:05.920
 starting point for for creating the probability distribution of where the wreckage occurred.

00:12:05.920 --> 00:12:12.880
 So anything outside the circle they assumed it was impossible to reach. The only the only other

00:12:12.880 --> 00:12:18.240
 piece of data were the four failed search attempts and then some of the debris found.

00:12:18.240 --> 00:12:28.160
 So one thing they did decide was to look at similar crashes where control was lost to analyze where

00:12:28.160 --> 00:12:39.680
 those crashes where the crashes landed compared to where the loss of control started and this

00:12:39.680 --> 00:12:46.000
 probability distribution the circular normal distribution was decided upon. So here you can see that

00:12:46.000 --> 00:12:52.320
 the center has a lot more a lot higher chance of finding the wreckage and as you as you go away from

00:12:52.320 --> 00:13:01.440
 the center the probability of finding the wreckage decreases a lot. So the next thing they looked at

00:13:01.440 --> 00:13:10.000
 was they noticed they had they had retrieved some dead bodies from the wreckage and so they thought

00:13:10.000 --> 00:13:19.200
 that they could calculate the backward drift on that particular day to find where where the crash

00:13:19.200 --> 00:13:24.480
 might have occurred. So if they found bodies at a particular location they can kind of work backwards

00:13:24.480 --> 00:13:31.840
 from that in order to find where the where the initial crash occurred. So here you can see the

00:13:31.840 --> 00:13:39.360
 probability distribution based off of the the backward drift model. So here you see the darker

00:13:39.360 --> 00:13:47.280
 darker colors have a higher probability of finding the location. And so with all these pieces of

00:13:47.280 --> 00:13:55.200
 data. So with the with that circular 40 nautical mile uniform distribution with the with that circular

00:13:55.200 --> 00:14:03.440
 normal distribution of comparing similar crashes as well as with the back backward drift

00:14:03.440 --> 00:14:08.400
 distribution they they're able to combine all three of these pieces

00:14:08.400 --> 00:14:16.160
 in order to come up with a final prior distribution of where the wreckage occurred.

00:14:16.160 --> 00:14:23.360
 And so this is what this is what the final model they came upon. So here you can see it's it's has

00:14:23.360 --> 00:14:30.160
 that 40 nautical mile radius circle. It has that that darker center which indicates a higher

00:14:30.160 --> 00:14:38.800
 probability because of the the crash similarity. And then you hear you also see along this along this

00:14:38.800 --> 00:14:50.560
 line has a slightly higher probability due to the backward drift distribution. So the next thing is

00:14:50.560 --> 00:14:58.240
 since they they had performed searches they they were able to they they decided to incorporate

00:14:58.240 --> 00:15:04.240
 that data the data from those searches into their their new distribution. So here you can see

00:15:04.240 --> 00:15:10.400
 places where they searched initially. And so if you think about it you can assume that well if you

00:15:10.400 --> 00:15:16.160
 search for something there's a good chance you'll find it but not necessarily. And so anywhere where

00:15:16.160 --> 00:15:23.200
 they searched the probability of it finding it there is greatly reduced. It's not zero because obviously

00:15:23.200 --> 00:15:29.840
 you can look for something and and miss it. But it kind of reduces the probability that we would

00:15:29.840 --> 00:15:37.280
 expect to find it in those those already searched locations. And so this is this is the posterior

00:15:37.280 --> 00:15:46.880
 distribution or distribution after counting into after counting observations made. And here we can

00:15:46.880 --> 00:15:53.920
 see kind of these cutouts of where the previous searches occurred. So this is this is the the final

00:15:53.920 --> 00:16:01.440
 distribution they went off of to perform the subsequent search. In the end the wreckage was was found

00:16:01.440 --> 00:16:08.320
 at a point close to the center here thus validating this methodology. We can see the power of this

00:16:08.320 --> 00:16:13.520
 Bayesian search methodology in this in the way that we could take information from all the sources

00:16:13.520 --> 00:16:21.520
 we had. We could draw analogies to similar situations. We can quantify these combine them into a

00:16:21.520 --> 00:16:28.800
 model and then also update our model according to each observation we make. And I think there's a lot

00:16:28.800 --> 00:16:34.720
 of similarities to be drawn with searching on a computer. In the sense that when we search for

00:16:34.720 --> 00:16:40.640
 something there's often a oftentimes a story we we kind of have as to like what what search terms

00:16:40.640 --> 00:16:46.480
 exist where we expect to find the file. So for example if you're implementing a new feature you'll

00:16:46.480 --> 00:16:52.560
 often have some search terms in mind that you think will be relevant. Some search terms you might

00:16:52.560 --> 00:16:58.400
 you might think they have a possibility of being relevant but maybe you're not sure. There's some

00:16:58.400 --> 00:17:06.000
 there's some directories where you know that they're not relevant. There's some there's other criteria

00:17:06.000 --> 00:17:12.480
 like well you know that maybe somebody in particular worked on this code. And so what if you could

00:17:12.480 --> 00:17:19.520
 you can incorporate that information like I know this author he's always working on this this

00:17:19.520 --> 00:17:25.280
 this feature. So what if what if I just give the code the files that this person works on more

00:17:25.280 --> 00:17:33.360
 more you know a higher probability than once he doesn't work on. Or maybe you think that this

00:17:33.360 --> 00:17:40.560
 this was a this is a piece of this is a file that's committed to often. You think that maybe the

00:17:40.560 --> 00:17:48.160
 the amount of times that of commits that receives kind of should change your the probability of

00:17:48.160 --> 00:17:53.840
 this file being relevant. And so that's where search piece search comes in. So it's it's aim is to

00:17:53.840 --> 00:17:59.280
 be framework in order to incorporate all these sorts of different prior information

00:17:59.280 --> 00:18:06.800
 into your searching process. So you're able to say things like I want to I want files authored by

00:18:06.800 --> 00:18:13.280
 this user to be given higher probability. I want this author to be given a lower priority. I know

00:18:13.280 --> 00:18:20.880
 this author never works on this code. So if if if he has a commit then lower its probability or you can

00:18:20.880 --> 00:18:29.040
 specify specific paths or you can specify multiple search terms weighing weighing one different

00:18:29.040 --> 00:18:35.200
 ones according to according to how you think those terms should be relevant.

00:18:35.440 --> 00:18:44.320
 So with piece search we're able to incorporate information from multiple sources. So here for

00:18:44.320 --> 00:18:52.320
 example we have a prior of type git author. And we're we're looking for all of the files that are

00:18:52.320 --> 00:18:59.680
 committed to by Lars. And so the more commits he has the higher probability is given to that file.

00:19:00.400 --> 00:19:06.320
 So suppose there's a feature I know he worked on, but I don't know the file or necessarily even

00:19:06.320 --> 00:19:14.080
 key terms of it. Well with this I can I can incorporate that information. And so let's search again. Let's

00:19:14.080 --> 00:19:23.920
 add let's add display. And let's see what responses we get back here. And we can add as many of these

00:19:23.920 --> 00:19:30.080
 these criteria as we want. We can even specify that the title of the file name should should be a

00:19:30.080 --> 00:19:38.480
 certain type. So let's say we only concerned about c files. So we add the the file name should be

00:19:38.480 --> 00:19:49.200
 should contain dot c in it. So with this now we notice that all of the c files containing

00:19:49.200 --> 00:19:57.920
 display authored by Lars should be given higher probability. And we can continue to add these

00:19:57.920 --> 00:20:07.920
 priors as we feel fit. So the workflow that I found helps when searching is that you'll you'll add

00:20:07.920 --> 00:20:13.680
 criteria. You'll see some good results come up and some bad results come up. And so you'll

00:20:13.680 --> 00:20:19.440
 often find a pattern in those bad results like oh I don't want test files or this this director

00:20:19.440 --> 00:20:25.120
 isn't relevant or or something like that. And then you can update your your prior distribution.

00:20:26.640 --> 00:20:32.320
 Adding its criteria and then rerun it and then it will get different probabilities for the files.

00:20:32.320 --> 00:20:38.480
 And so in the end you'll you'll have a list of results that's tailor made to the thing you're

00:20:38.480 --> 00:20:45.440
 searching for. And so there's a couple of other features I want to go through. So one thing is that

00:20:45.440 --> 00:20:55.680
 each of these priors they have a you can specify the importance. In other words how

00:20:56.480 --> 00:21:02.160
 how important is is this is this particular piece of information to your to your search.

00:21:02.160 --> 00:21:07.520
 So here everything is is of importance medium. But let's say I really care about something having

00:21:07.520 --> 00:21:14.880
 that we're displaying it. So I'm going to change its importance to instead of medium I'll change

00:21:14.880 --> 00:21:20.800
 it's I'll change its importance to high. And so it so what it what that does essentially is

00:21:22.160 --> 00:21:28.480
 things that don't have displaying it are given a much bigger penalty and things with with the word

00:21:28.480 --> 00:21:36.160
 displaying it is are rated much higher. And so with this we're able to kind of fine tune our

00:21:36.160 --> 00:21:45.360
 different are the results that we get. Another thing you can do is that you can you can add the complement

00:21:46.320 --> 00:21:52.480
 or the inverse of certain queries. So let's say you want to search or display but you don't want it

00:21:52.480 --> 00:21:58.880
 to contain the word frame. So with the complement option on when we create this this search prior

00:21:58.880 --> 00:22:04.320
 now it's going to be searching for frame but instead of increasing the search score it's going to

00:22:04.320 --> 00:22:06.880
 it's going to decrease it if it contains the word frame.

00:22:11.600 --> 00:22:17.600
 Yeah so here things related to frame are kind of deprioritized and we can also say that we really

00:22:17.600 --> 00:22:23.520
 don't want the search to contain the word frame by increasing its importance. And so with these

00:22:23.520 --> 00:22:32.080
 with all these composable pieces we can we can create kind of a search that's tailor made to our needs.

00:22:32.080 --> 00:22:36.400
 So that concludes this talk. There's a lot more I could talk about with regards to

00:22:36.400 --> 00:22:40.720
 these search so definitely follow the project if you're interested. Thanks for watching and I hope you

00:22:40.720 --> 00:22:42.800
 enjoy the rest of the conference.

